# Another Build System
You are right to be skeptical. Why do we need yet another competing solution for building our code that will fragment our community even further? I have spent a majority of my career as a software engineer trying to convince others not to reinvent what can instead be can borrowed and extended from others. I think that is why it has taken me so long to put down in words my reasoning for creating this project. However, I truly believe that we are at a unique point in the life span of C++ that we can finally create a build system that resolves all of the major issues holding back C++ from being a best in class experience for everyone, from the programmer writing their very first "Hello World!" to the most weathered of coders.

With C++ 20 coming out this year we will finally be getting our hands on the long awaited (and controversial) Modules support. It is this feature that will allow C++ builds to finally have a clean binary separation between individual projects that will open the door to fixing many of the problems present in C++ today. At the same time, migrating our code to support Modules will require a substantial amount of work, which means this is the ideal time to consider a major shift in what tooling we use as a community. In this post I outline a general overview of the key issues present in building and sharing our code today and then present a new build system that leverages Modules at its core to create a new way to collaborate around the open source community.

## Sharing Code
At its core C++ is quite simple to build. The most basic executable can be created using one source file with a single call to the compiler of your choice in your favorite command line. However, as you pile on a more and more requirements the complexity becomes unmanageable and the need to automate our builds becomes apparent. Beyond the normal dependency graph complexity that will be present in any programming language, C++ has extra issues that make it especially hard to build and even more so to share those builds with others in the community. There are three primary issues that make C++ a hard language to collaborate around; it has multiple compiler implementations, it is a compiled language and it inherited the legacy of the C preprocessor.

### Specification
Unlike many other languages out there today that have both a language specification and a single first party implementation, the C++ language is primarily a specification, and has no first party compiler. This means that we get to have multiple compilers from different vendors that allows for targeting many different architectures. It also means that if I want to share my code with the C++ community as a whole I have to verify that I do not have any platform specific logic and have a unique setup for each compiler to ensure the build works correctly. This is not too difficult of a problem to handle with a good build system, but does require some integration work to support new compiler vendors. This is also perhaps where C++ has made the greatest strides with the continued evolution of the standard library.

### Compiled
The overhead of having many different compiler implementations is compounded by the fact that C++ is compiled directly to the assembly for the target machine that will execute the code. C++ puts no constraints on how a compiler does this mapping which means that the [Application Binary Interface (ABI)](https://en.wikipedia.org/wiki/Application_binary_interface) between two compilers (and sometimes between versions of the same compiler) are not compatible with each other. This means we have to ensure that all of our objects were generated using the same compiler or take special care to work around this issue using strict design practices. There have been multiple approaches in the past to combat binary compatibility issues when sharing C++ code.

Perhaps the oldest way to share native code is to pre-build the binaries and distribute a single dynamic library along with a set of public header files. One way to get around the binary compatibility issue is to only expose a C style public binary layer that takes advantage of the fact that C does have a standardized naming convention. This requires that all C++ implementation code be wrapped in a public C layer and if a client wishes to use modern C++ concepts the C layer can then be wrapped in another C++ layer that is compiled within the consumer project. A second pattern that allows for the distribution of pre-built native binaries is to expose a single C style entry point and from then on use only interfaces when communicating across the boundary (Note: beware of exceptions or standard library objects passing over the boundary!). While not technically a requirement that C++ interface definition have a standard ABI, Microsoft has effectively standardized this approach through the sheer number of projects that utilize it through [COM](https://en.wikipedia.org/wiki/Component_Object_Model). Both of these approaches will produce fully compatible binaries that can be distributed to others, however the overhead of either approach is often not worth the effort unless your shared component is very large (i.e. Boost).

Another approach to binary compatibility is to have no binaries at all. To work around binary compatibility issues some communities of smaller projects have started embedding both the definition and implementation into header only libraries. When including the headers in your project you are effectively building the project for them. Due to the constraint that you must now place all of your source in your public headers these headers can grow unwieldy and will be unmanageable for large projects. These large headers can also have a negative impact on build performance as re-parsed multiple times in every translation unit that consumes them.

A relatively new approach to consuming external dependencies is through package managers. A package manager distributes either the raw source along with the build definition required to integrate with your project and as long as your two systems are compatible it will automatically inject the child dependency into your build or pre-download the pre-built binaries that were carefully cataloged to have the same compiler, architecture and configurations. This approach works well, but does require that the package manager is able to generate the required build definitions to be used by consumers or be directly integrated within a build system.

### Preprocessor
The preprocessor is, until now, a point of failure that could not be protected against by any build system when integrating with external source. Until C++ 20 the only way to share a symbol definition was to place it in a header file and have that header file be included by both the implementation and all of the translation units that wish to use it. This can lead to unforeseen compatibility issues when a header is included with a different set of preprocessor definitions defined from what were present when compiling the implementation. At best this will result in a compiler or linker error, and at worst you will have a fun [one definition rule](https://en.wikipedia.org/wiki/One_Definition_Rule) violation to track down! This is where Modules shine, and the primary driver behind why I believe we can finally make C++ the best open source, collaborative language!

Another major issue with sharing code between different projects is incompatible language standards. In general it is straightforward to pull source that targets an earlier versions of the language into a project with a newer version (i.e unless the `C++ 11` code uses a removed standard library feature it will compile fine with a `C++ 14` compiler). This means that header only libraries must have compile time checks for different language versions and pre-compiled libraries already stay away from the standard library.

## Proposal
It is not enough to say that Modules will solve all of our problems. We will also have to define clear priorities for a collaboration first build system. 

### Requirements
The set of requirements cannot be compromised. They do not necessarily have a priority order since they cannot conflict with each other, if the concepts are incompatible then the final system would be deemed a failure.

1) Reproducible - Core to any build system is the requirement that builds be deterministic and reproducible. This design requirement is highest on the list because no matter how well a system is designed and implemented, teams will not be able to utilize it unless they can trust that it will always produce the same result no matter who builds it and when.

2) Extensible - A build system should be able to support the requirements of all projects, to support this it must have an extensibility framework that allows build architects to write their own custom build logic.

3) Isolation - This is a unique requirement as a result of the above overview of sharing C++ code today. Isolated builds means that one project cannot influence or be influenced by another build except through explicit structured channels.

### Goals
While the goals are not hard requirements they are always kept front of mind when making any design or implementation decision. These items are in priority order.

1) Collaborative - Writing code is very rarely done in isolation. The largest goal for this build system is to be able to work seamlessly within a team and with external dependencies.

2) Simple - When fulfilling the above requirements the highest priority is always simplicity and usability. This means that the standard user will get the best experience possibly for both setup and usage. Some extra complexity is allowed in exchange for performance gains in the internal implementation and the extensibility framework.

3) Fast - The inner developer loop is very important to the productivity of engineers. To this end the build system should focus heavily on the performance of an incremental build and to a lesser extend ensure the full build is as fast as possible.

## Design
The primary design consists five key components that work together to fulfill the requirements for a C++ build system centered around collaborating around our shared code. The Command Line interface (CLI), the build definition, the build engine, the operation evaluation engine and the package repository.

### CLI
The Command Line Interface is the first thing a user sees when they interact with the build system. The CLI is primarily there to take user input through a set of parameters and flags to pass temporary configuration values into the build execution. While important it is fairly straightforward to design and will be left to open to evolve through use. 

### Definition
The build definition, which will call a Recipe, is how the user will configure their project through a declarative configuration file. The Recipe file will utilize the [toml](https://github.com/toml-lang/toml) language as a clean, human readable configuration definition that supports a core set of data types. The file can be thought of as a simple property bag for getting shared parameters passed into the build system. There are a few "known" property values that will be used within the build engine itself, however the entire contents will be provided as initial input to the build engine.

### Engine
The build Engine has two jobs; to recursively build all transitive dependencies, and performing the registration and execution of build tasks that make up the core build functionality. All build functionality will be contained in Tasks. A Build Task will consist of a unique name, lists of other tasks that must be run before or after it, and a single Execute entry point. These build Tasks will be registered through Dynamic Libraries that expose a single predifined C method. The build Tasks will then communicate with the build Engine itself through a strict interface layer to provide a compatible ABI that will allow the CLI executable that contains the build Engine implementation to work with the source compiled development dependencies. This work can be broken down into five phases.

1. **Parse Recipe** - The Recipe toml file is read from disk and parsed into a property bag.
2. **Build Dependencies** - The Engine will use the known property lists "Dependencies" and "DevDependencies" to recursively builds all transitive runtime and development dependencies starting at phase one of the build. The Engine will perform the task of passing open a communication channel between parent and children project builds to allow for passing shared parameters down and output objects back up. 
3. **Build Extensions** - The engine will then invoke the predefined C method that is exported from all Extension DLL. The engine will discover these DLLs from the Development Dependencies list as well as a single predefined Extension DLL that is distributed with the CLI executable and contains the Tasks that can take a standard Recipe definition and convert it into the required compile commands with the initial known set of Compiler implementations.
4. **Run Tasks** - The build engine will invoke all registered build tasks in their defined order. The Tasks can influence each other by reading and setting properties on the active state. A build task should not actually perform any commands itself, it will instead generate build Commands which are self contained operation definitions with input/output files.
5. **Run Commands** - The final stage of the build is to execute the build commands that were generated from the build tasks. These commands contain the input and output files that will be used to perform incremental builds. (Note: There is currently a very simple time-stamp based incremental build that relies on the compiler generated include list. There is an open question of which project will be used to replace this temporary solution. The current best choices are either [BuildXL](https://github.com/microsoft/BuildXL) or possibly [Ninja](https://github.com/ninja-build/ninja)).

### Repository
You may have noticed that nothing about the build explicitly knows about integrating with a public feed of packages. The key concept is that because each individual projects build is isolated and self contained a dependency reference can easily be migrated from a direct directory reference for local projects to a name/version pair that will be resolved to a published snapshot of a public project. The CLI application can then consume a rest API from a service that allows for users to install other projects and publish the code they would like to share with ease.

## Summary
It would take a huge amount of time and effort to transition the entire C++ community to a new ecosystem of build tooling. However, C++ 20 presents a unique opportunity in that migrating to replace preprocessor includes with modules is a non-trivial breaking change. I believe that by transitioning at the same time to a build system that was designed explicitly for use in this new world we can lessen this work and finally get to a place where C++ is an amazing language for collaborating with others.

